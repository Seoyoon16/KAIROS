{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1L2HYV7-G32DLTAknraiQMUVWW1e08Ufh","timestamp":1706855478576},{"file_id":"128XY0igqNDLqiMv4wfc0rommNL8sUJau","timestamp":1706765336992},{"file_id":"1tfy_SdPspH6cX-T2F87RFkgFbEWBp5em","timestamp":1706765205930},{"file_id":"1PZ9f_y1n4gT3TnaemUBE0iseXsUkcrBK","timestamp":1706764600864}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# numpy 임포트"],"metadata":{"id":"ElfZHBpGku-Y"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"X5bqNLhYkUeo","executionInfo":{"status":"ok","timestamp":1706855604869,"user_tz":-540,"elapsed":2,"user":{"displayName":"‍박서윤[재학 / 프랑스.EU전공]","userId":"01832271897542160199"}}},"outputs":[],"source":["import numpy as np"]},{"cell_type":"markdown","source":["# 다층 퍼셉트론 클래스"],"metadata":{"id":"FCEREn_1k5ml"}},{"cell_type":"code","source":["class NeuralNetwork:\n","\n","    def __init__(self, layers, alpha=0.1):\n","        # 생성자: 신경망 초기화\n","        # alpha: 학습률\n","        self.W = [] # 가중치를 저장할 리스트\n","\n","        self.layers = layers  # list [ 2, 2, 2, 1]\n","        self.alpha = alpha\n","\n","        # 가중치 초기화: 입력 레이어부터 마지막 은닉 레이어까지의 가중치를 초기화\n","        for i in np.arange(0, len(layers) - 2): # 마지막 레이어는 가중치 적용x\n","          w = np.random.randn(layers[i]+1, layers[i+1]+1)\n","          self.W.append(w/np.sqrt(layers[i]))\n","\n","        # 마지막 은닉층에서 출력층으로의 가중치를 초기화합니다., 마지막 hidden - out 사이의 계수\n","        w = np.random.randn(layers[-2]+1, layers[-1])\n","        self.W.append(w/np.sqrt(layers[-2]))\n","\n","\n","\n","    def sigmoid(self, x):\n","        # 시그모이드(로지스틱스) 활성화 함수: 신경망의 각 노드에서 사용\n","\n","        return 1.0/(1+np.exp(-x))\n","\n","    def sigmoid_deriv(self, x):\n","        # 시그모이드 활성화 함수의 미분: 이는 역전파 시 그래디언트(기울기)를 계산할 때 사용\n","\n","        return x*(1-x)\n","\n","    # 신경망을 학습시키는 메서드입니다.\n","    # 입력 X = np.array([[ 0,0], [ 0,1], [1,0], [0,0]])\n","    # 타겟 y = np.array([[0], [1], [1], [0]])\n","    # epochs: 학습을 위해 데이터셋을 반복할 횟수입니다.\n","    def fit(self, X, y, epochs=1000):\n","        # 입력 데이터에 바이어스를 위한 1 추가\n","        X = np.c_[X, np.ones((X.shape[0]))]\n","\n","        for epoch in np.arange(0,epochs):\n","          for (x, target) in zip(X,y):\n","            self.fit_partial(x,target)\n","\n","\n","    def fit_partial(self, x, y):\n","        # 부분 학습 메서드: x는 단일 데이터 샘플, y는 해당 타겟 레이블\n","        A = [np.atleast_2d(x)]\n","\n","        # 순전파 단계: 입력 데이터가 네트워크를 통해 전파되면서 각 레이어의 출력 계산\n","        for  layers in np.arange(0, len(self.W)):\n","          net = A[layers].dot(self.W[layers])\n","          out = self.sigmoid(net)\n","          A.append(out)\n","\n","        # 역전파 단계: 신경망의 출력과 실제 타겟 값의 차이를 계산합니다.\n","        error = A[-1] - y\n","\n","        # 오류의 시그모이드 미분값을 사용하여 오류 신호 계산\n","        # 그래디언트\n","        D =[error * self.sigmoid_deriv(A[-1])]\n","\n","        # 모든 층에 대해 역전파를 수행합니다. 출력층부터 시작하여 입력층 방향으로 진행합니다.\n","        for  layers in np.arange(len(A) -2, 0, -1):\n","          delta = D[-1].dot(self.W[layers].T)\n","          delta = delta* self.sigmoid_deriv(A[layers])\n","          D.append(delta)\n","\n","\n","        # D 리스트를 뒤집습니다. 이렇게 하여 입력층에 가까운 오류부터 시작하게 됩니다.\n","        D = D[::-1]\n","\n","        # 가중치 업데이트: 모든 레이어에 대해 가중치를 업데이트합니다.\n","        for layer in np.arange(0, len(self.W)):\n","          self.W[layer] += -self.alpha *  A[layer].T.dot(D[layer])\n","\n","\n","    # 예측 메서드: 새로운 데이터에 대한 예측을 수행합니다.\n","    def predict(self, X):  # X = [0 1]\n","        # 입력 데이터를 최소 2차원 배열로 변환합니다.\n","        p = np.atleast_2d(X)\n","\n","        # 바이어스를 위한 1을 추가합니다.\n","        p = np.c_[p, np.ones((p.shape[0]))]\n","\n","        # 신경망을 통해 순전파를 수행하여 예측 결과를 계산합니다.\n","        for layer in np.arange(0, len(self.W)):\n","            p = self.sigmoid(np.dot(p, self.W[layer]))\n","        return p"],"metadata":{"id":"yBSseZxYkxxz","executionInfo":{"status":"ok","timestamp":1706855607503,"user_tz":-540,"elapsed":3,"user":{"displayName":"‍박서윤[재학 / 프랑스.EU전공]","userId":"01832271897542160199"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# 다층 퍼셉트론 클래스 객체 생성,\n","\n","1 hidden layer, learning rate 0.5"],"metadata":{"id":"k1cJfQjAlBYM"}},{"cell_type":"code","source":["nn = NeuralNetwork([2, 2, 1], 0.5)"],"metadata":{"id":"1aLUUj05k83o","executionInfo":{"status":"ok","timestamp":1706855612071,"user_tz":-540,"elapsed":303,"user":{"displayName":"‍박서윤[재학 / 프랑스.EU전공]","userId":"01832271897542160199"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# XOR 입력 데이터 정의, 출력값 정의, 트레이닝(back-propagation)"],"metadata":{"id":"GDwXPRyKlVCx"}},{"cell_type":"code","source":["X = np.array([[ 0,0], [ 0,1], [1,0], [1,1]])\n","y = np.array([[0], [1], [1], [0]])\n","nn.fit(X,y,100000)"],"metadata":{"id":"M8cTBvxWlR24","executionInfo":{"status":"ok","timestamp":1706855843571,"user_tz":-540,"elapsed":20627,"user":{"displayName":"‍박서윤[재학 / 프랑스.EU전공]","userId":"01832271897542160199"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# feed-forward 예측"],"metadata":{"id":"BoFZxCF2le1Z"}},{"cell_type":"code","source":["for (x, target) in zip(X,y):\n","  pred = nn.predict(x)[0][0]\n","  step  = 1 if pred > 0.5 else 0\n","  print(target[0], pred,step)"],"metadata":{"id":"A1RpHkpblbBB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1706855848659,"user_tz":-540,"elapsed":326,"user":{"displayName":"‍박서윤[재학 / 프랑스.EU전공]","userId":"01832271897542160199"}},"outputId":"97e122da-3ecd-472c-d41d-ab59ba30c92c"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["0 0.005007241723071168 0\n","1 0.9966667170697034 1\n","1 0.9941813292878547 1\n","0 0.006004901673240968 0\n"]}]}]}